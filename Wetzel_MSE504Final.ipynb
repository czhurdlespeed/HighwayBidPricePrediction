{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff0d235-14ad-4580-84ec-6fde6fbdb0c0",
   "metadata": {},
   "source": [
    "# MSE504 Final Project Submission - NCDOT Contract Unit Prices Prediction\n",
    "## Dr. Kalinin \n",
    "## Jon Calvin Wetzel\n",
    "### December 9, 2024\n",
    "\n",
    "#### Notebook Table of Contents\n",
    "- Problem Statement\n",
    "- Data Collection\n",
    "- Data Preprocessing\n",
    "- Unit Price Prediction - KNN Regression with Kernel PCA\n",
    "- Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc82b6a-228a-421c-9fa5-ee37755131fd",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbd821-100f-4b1f-94d5-6746f606945a",
   "metadata": {},
   "source": [
    "Across the United States, state departments of transportation (DOTs), such as NCDOT, manage highway infrastructure and transporation related construction projects. Such projects are drawn up and detailed by state engineers so that state contractors can adequately follow a construction plan. Before construction can begin, state DOTs release the project plans for statewide contractors to bid on, where the lowest bidder receives/wins the contract. Although a contractor would like to win the bid by offering the lowest price for job completion, the contractor does not want to bid signficantly lower than their competition as that represents money left on the table i.e., the dollar difference between the first and second place bidders. Futhermore, consistently bidding too low drives the market prices down and can result in reduced future profits.\n",
    "\n",
    "Consequently, contractors continuously analyze the bidding history and behavior of their competition, as it relates to the current job they would like to bid on, with the goal to bid lower than their competition and simultaneously minimize money left on the table. Given the publically available information regarding these historic, current, and upcoming contracts, one can attempt to use machine learning to help contractors win more work at the right price while minimizing money left on the table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973569d6-eaf0-4ec2-8c48-209fa23c75e6",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af8a12-66d7-4112-82da-d1743b1b81cc",
   "metadata": {},
   "source": [
    "Every DOT hosts a website that records and publishes contract information for statewide projects. Included in this information is an itemized list of items required and bid on for each project. Each contractor must charge a price for each item within the project. The contractor's proposed price is then multiplied by the payitem's quantity, which is then summed across all payitems to produce the contractor's final total bid amount for the contract.  \n",
    "\n",
    "[NCDOT Website](https://connect.ncdot.gov/letting/Central%20Letting/Forms/BidTabs.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02094bf3-8887-4e2e-b899-5150a8746f72",
   "metadata": {},
   "source": [
    "__Figure 1: NCDOT Page Containing Contract Information__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6e79b-83ad-407d-b443-6cb77827c1b9",
   "metadata": {},
   "source": [
    "![NCDOTWebsite](imgs/NCDOTpage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602a1e1-c928-44c1-b550-ac78b531374b",
   "metadata": {},
   "source": [
    "As depicted in the image, contract information dates back to 2013. Futhermore, each month's letting (collection of contracts for the month) containes an excel file with the pay items, quantities, and unit prices (data) for each project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6197e-ce77-4a0d-8da7-4f934f57718b",
   "metadata": {},
   "source": [
    "__Figure 2: April 15, 2014 Letting Block with Link to Excel File Containing Data for Contracts__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159059c-aa83-4a8f-9bc5-28d9513cdc72",
   "metadata": {},
   "source": [
    "![Excel Link](imgs/NCDOTexcellink.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8a3ce-aa87-4864-978c-4b0b26ce77c3",
   "metadata": {},
   "source": [
    "Given the large number of contracts and pagination format of the NCDOT website, the data collection process would be tedious. It can be automated by using a [Selenium](https://www.selenium.dev/) script to download the excel files over a specified date range. \n",
    "\n",
    "The Selenium module permits interaction with a web browser via code, which can replicate user interaction with a website e.g., clicking links, navigating the pages, etc. This can signficantly enhance the data collection and data update workflow!\n",
    "\n",
    "The code below utilizes Selenium to navigate the NCDOT website and download the excel files into a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1edb07fe-2985-4d27-b820-324f1603cf44",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoSuchElementException, TimeoutException\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    # Set up download directory\n",
    "    download_dir = os.path.join(os.getcwd(), \"ExcelFiles\")\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    # Configure Chrome options for downloading\n",
    "    options.add_experimental_option(\n",
    "        \"prefs\",\n",
    "        {\n",
    "            \"download.default_directory\": download_dir,\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True,\n",
    "            # Add these lines to handle Excel files\n",
    "            \"plugins.always_open_pdf_externally\": True,  # Force download for PDFs\n",
    "            \"download.open_pdf_in_system_reader\": False,\n",
    "            # MIME types for Excel files\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"download.default_directory\": download_dir,\n",
    "            \"safebrowsing.enabled\": True,\n",
    "            \"profile.content_settings.exceptions.automatic_downloads.*.setting\": 1,\n",
    "            \"profile.default_content_setting_values.automatic_downloads\": 1,\n",
    "            \"download.prompt_for_download\": False,\n",
    "            # Add MIME type handling\n",
    "            \"plugins.plugins_list\": [\n",
    "                {\"enabled\": False, \"name\": \"Chrome PDF Viewer\"}\n",
    "            ],\n",
    "            \"download.extensions_to_open\": \"\",\n",
    "            \"profile.default_content_settings.popups\": 0,\n",
    "            \"download.default_directory\": download_dir,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Add additional arguments\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%m/%d/%Y\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_excel_file(driver):\n",
    "    try:\n",
    "        # Find the Excel link\n",
    "        excel_link = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, \"a.ms-listlink.ms-draggable[app='ms-excel']\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Get the href (download URL)\n",
    "        download_url = excel_link.get_attribute(\"href\")\n",
    "        file_name = excel_link.text\n",
    "        print(f\"Downloading: {file_name}\")\n",
    "\n",
    "        # Use JavaScript to trigger download\n",
    "        js_script = f\"\"\"\n",
    "        var link = document.createElement('a');\n",
    "        link.href = '{download_url}';\n",
    "        link.download = '{file_name}.xls';\n",
    "        document.body.appendChild(link);\n",
    "        link.click();\n",
    "        document.body.removeChild(link);\n",
    "        \"\"\"\n",
    "        driver.execute_script(js_script)\n",
    "\n",
    "        time.sleep(3)  # Wait for download to start\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading Excel file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_all_page_urls(driver, base_url):\n",
    "    urls = [base_url]\n",
    "    try:\n",
    "        while True:\n",
    "            # Try to find specifically the next button (not previous) by looking for right arrow image\n",
    "            next_button = driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                \"a.ms-commandLink.ms-promlink-button.ms-promlink-button-enabled img.ms-promlink-button-right\",\n",
    "            ).find_element(By.XPATH, \"..\")\n",
    "\n",
    "            if next_button.is_displayed() and next_button.is_enabled():\n",
    "                next_button.click()\n",
    "                time.sleep(2)\n",
    "                # Get the URL of the new page\n",
    "                current_url = driver.current_url\n",
    "                urls.append(current_url)\n",
    "                print(f\"Found page {len(urls)}: {current_url}\")\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Total pages found: {len(urls)}\")\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def navigate_letting_pages():\n",
    "    driver = setup_driver()\n",
    "    cutoff_date = datetime(2022, 1, 1)\n",
    "    main_window = None\n",
    "    base_url = (\n",
    "        \"https://connect.ncdot.gov/letting/Central%20Letting/Forms/BidTabs.aspx\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # First get all page URLs\n",
    "        print(\"Discovering all pages...\")\n",
    "        driver.get(base_url)\n",
    "        time.sleep(3)\n",
    "        page_urls = get_all_page_urls(driver, base_url)\n",
    "        print(f\"Found {len(page_urls)} pages\")\n",
    "\n",
    "        # Now process each page\n",
    "        for page_num, url in enumerate(page_urls, 1):\n",
    "            print(f\"\\nProcessing page {page_num} of {len(page_urls)}\")\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            if main_window is None:\n",
    "                main_window = driver.current_window_handle\n",
    "\n",
    "            # Get all rows on current page\n",
    "            table_rows = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.CSS_SELECTOR, \"tr.ms-itmhover\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Process each row\n",
    "            for row in table_rows:\n",
    "                try:\n",
    "                    date_cell = row.find_element(\n",
    "                        By.CSS_SELECTOR, \"td[role='gridcell'] span.ms-noWrap\"\n",
    "                    )\n",
    "                    date_text = date_cell.get_attribute(\"title\")\n",
    "                    letting_date = parse_date(date_text)\n",
    "\n",
    "                    if letting_date and letting_date > cutoff_date:\n",
    "                        link = row.find_element(\n",
    "                            By.CSS_SELECTOR, \"a.ms-listlink.ms-draggable\"\n",
    "                        )\n",
    "                        print(f\"Processing: {link.text} - Date: {date_text}\")\n",
    "\n",
    "                        # Open in new tab using JavaScript\n",
    "                        href = link.get_attribute(\"href\")\n",
    "                        driver.execute_script(\n",
    "                            \"window.open(arguments[0]);\", href\n",
    "                        )\n",
    "\n",
    "                        # Switch to new tab\n",
    "                        driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "                        if download_excel_file(driver):\n",
    "                            print(\"Successfully downloaded Excel file\")\n",
    "\n",
    "                        # Close tab and switch back\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(main_window)\n",
    "                        time.sleep(2)\n",
    "                    else:\n",
    "                        print(f\"Skipping: Date {date_text} is before cutoff\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53d88a-1f2d-4dde-8fbc-ca5c162aef09",
   "metadata": {},
   "source": [
    "__Run the Selinium Script__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba8763-acf8-42ef-ab80-c1a7fc578356",
   "metadata": {},
   "outputs": [],
   "source": [
    "navigate_letting_pages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07168a-4fec-40ea-8ea4-afb9cbd5ee1f",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ee489-451c-4932-b27d-6853eab1cc62",
   "metadata": {},
   "source": [
    "Once the excel files are downloaded to a local directory, they can be opened and saved to csv files using the csv module. In most scenarios, pandas can be used to complete this task. However, the complex nature of the excel sheet layouts requires a custom script for parsing the excel files and extracting the information necessary to create the individual contract csv files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a683792-dc7b-4be6-9a83-c9785da91326",
   "metadata": {},
   "source": [
    "__Figure 3: Example Excel Sheet with Raw Project Data. Note the Contract Total Cells at the Bottom. Each Excel Sheet Contains multiple projects like this.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1952b-fa0c-417d-abce-57f2ccd0389a",
   "metadata": {},
   "source": [
    "![ExcelSheet](imgs/ExcelSheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db6baf-c8f4-4656-be08-02c92c279d56",
   "metadata": {},
   "source": [
    "The code below reads the excel sheets, extracts the raw data for each contract, and writes the information to a csv file for each contract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5d646-6394-4c83-a5ac-e551d392a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_excel(file_path: Path) -> pd.DataFrame:\n",
    "    return pd.read_excel(file_path, header=None)\n",
    "\n",
    "\n",
    "def desired_general_columns(\n",
    "    general_contract_info: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    desired_columns = general_contract_info.iloc[\n",
    "        :,\n",
    "        [2, 5, 9, 12, 13, 14, 16, 18, 19, 20, 21, 22],\n",
    "    ].copy()\n",
    "    desired_columns.rename(\n",
    "        columns={\n",
    "            2: \"Date\",\n",
    "            5: \"Proposal\",\n",
    "            9: \"County\",\n",
    "            12: \"Proposal Type\",\n",
    "            13: \"Proposal Description\",\n",
    "            14: \"Item\",\n",
    "            16: \"Line Item\",\n",
    "            18: \"Item Type\",\n",
    "            19: \"Item Description\",\n",
    "            20: \"Item Description Subset\",\n",
    "            21: \"Quantity\",\n",
    "            22: \"Unit\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    desired_columns[\"Date\"] = pd.to_datetime(desired_columns[\"Date\"])\n",
    "    desired_columns[\"Line Item\"] = pd.to_numeric(\n",
    "        desired_columns[\"Line Item\"], errors=\"coerce\"\n",
    "    ).fillna(0)\n",
    "    # Get mask of original empty values\n",
    "    empty_mask = desired_columns[\"Line Item\"] == 0\n",
    "    # Forward fill values\n",
    "    filled_values = desired_columns[\"Line Item\"].ffill()\n",
    "    # Where values were empty, add 1 to previous value\n",
    "    desired_columns.loc[empty_mask, \"Line Item\"] = filled_values[empty_mask] + 1\n",
    "    desired_columns[\"Line Item\"] = desired_columns[\"Line Item\"].astype(int)\n",
    "    return desired_columns\n",
    "\n",
    "\n",
    "def contractor_per_payitem(contract_df: pd.DataFrame) -> Dict:\n",
    "    contract_df = contract_df.reset_index(drop=True)\n",
    "    contractor_payitems = contract_df.iloc[:, 24:]\n",
    "    contract_information = dict()\n",
    "    proposal_id = None\n",
    "    for row in range(0, contractor_payitems.shape[0]):\n",
    "        for col in range(0, contractor_payitems.shape[1] - 1, 5):\n",
    "            general_contract_row_info = desired_general_columns(\n",
    "                contract_df.iloc[row, :24].to_frame().T\n",
    "            )\n",
    "            line_item = general_contract_row_info.at[row, \"Line Item\"]\n",
    "            contractor = contractor_payitems.iloc[row, col]\n",
    "            if pd.isna(contractor) or contractor == \"\" or contractor == \" \":\n",
    "                continue\n",
    "            unit_price = contractor_payitems.iloc[row, col + 2]\n",
    "            extension = contractor_payitems.iloc[row, col + 3]\n",
    "\n",
    "            if line_item not in contract_information:\n",
    "                contract_information[line_item] = dict()\n",
    "\n",
    "            contract_information[line_item][contractor] = {\n",
    "                \"unit_price\": unit_price,\n",
    "                \"extension\": extension,\n",
    "                \"general_contract_row_info\": general_contract_row_info.reset_index(\n",
    "                    drop=True\n",
    "                ),\n",
    "            }\n",
    "            if proposal_id is None:\n",
    "                proposal_id = str(\n",
    "                    general_contract_row_info.at[row, \"Proposal\"]\n",
    "                ).strip()\n",
    "\n",
    "    return contract_information, proposal_id\n",
    "\n",
    "\n",
    "def format_and_write_to_csv(\n",
    "    contract_information: Dict, proposal: str, path=Path.cwd() / \"nc_csv\"\n",
    ") -> None:\n",
    "    HEADER = [\n",
    "        \"Proposal\",\n",
    "        \"Proposal Type\",\n",
    "        \"Proposal Description\",\n",
    "        \"Line\",\n",
    "        \"Item\",\n",
    "        \"Item Type\",\n",
    "        \"Item Description\",\n",
    "        \"Item Description Subset\",\n",
    "        \"Quantity\",\n",
    "        \"Unit\",\n",
    "        \"Vendor Name\",\n",
    "        \"Unit Price\",\n",
    "        \"Extension\",\n",
    "        \"County\",\n",
    "        \"Date\",\n",
    "    ]\n",
    "    Path.mkdir(path, exist_ok=True)\n",
    "\n",
    "    with open(path / f\"{proposal}.csv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(HEADER)\n",
    "        for line_item, contractors in contract_information.items():\n",
    "            for contractor, data in contractors.items():\n",
    "                general_contract_row_info = data[\"general_contract_row_info\"]\n",
    "                row = [\n",
    "                    general_contract_row_info.at[0, \"Proposal\"],\n",
    "                    general_contract_row_info.at[0, \"Proposal Type\"],\n",
    "                    general_contract_row_info.at[0, \"Proposal Description\"],\n",
    "                    general_contract_row_info.at[0, \"Line Item\"],\n",
    "                    general_contract_row_info.at[0, \"Item\"],\n",
    "                    general_contract_row_info.at[0, \"Item Type\"],\n",
    "                    general_contract_row_info.at[0, \"Item Description\"],\n",
    "                    general_contract_row_info.at[0, \"Item Description Subset\"],\n",
    "                    general_contract_row_info.at[0, \"Quantity\"],\n",
    "                    general_contract_row_info.at[0, \"Unit\"],\n",
    "                    contractor,\n",
    "                    data[\"unit_price\"],\n",
    "                    data[\"extension\"],\n",
    "                    general_contract_row_info.at[0, \"County\"],\n",
    "                    general_contract_row_info.at[0, \"Date\"].strftime(\n",
    "                        \"%m/%d/%Y\"\n",
    "                    ),\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "def process_letting_group_excel(raw_df: pd.DataFrame) -> None:\n",
    "    valid_rows = raw_df[16].notna()\n",
    "    unique_contracts = raw_df[4].unique().tolist()\n",
    "    for i in unique_contracts:\n",
    "        contract_df = raw_df[valid_rows & (raw_df[4] == i)]\n",
    "        assert contract_df.shape[0] > 0, \"Contract dataframe is empty\"\n",
    "        contract_info, proposal = contractor_per_payitem(contract_df)\n",
    "        format_and_write_to_csv(contract_info, proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022dfd9e-9751-4751-b72b-37286759c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_lettings_directory_path = Path.cwd() / \"ExcelFiles\"\n",
    "shutil.rmtree(Path.cwd() / \"nc_csv\", ignore_errors=True)\n",
    "for file_path in excel_lettings_directory_path.glob(\"*.xls\"):\n",
    "    raw_excel_df = read_excel(file_path)\n",
    "    process_letting_group_excel(raw_excel_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a5b0f-ecad-4e39-8a4d-a1c531e3fa0e",
   "metadata": {},
   "source": [
    "The resulting csv file for each contract looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5800ba-2f39-4c73-9d75-53fffd2c205b",
   "metadata": {},
   "source": [
    "__Figure 4: CSV file for an individual contract__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84f0af-2ccb-49a3-a70c-a66fde52f3d8",
   "metadata": {},
   "source": [
    "![ContractCSVFile](imgs/CSVFile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bba52-89c4-474b-92bb-39795bf460e1",
   "metadata": {},
   "source": [
    "With a consistent csv format and data across all contracts, data analysis can take place. After discussing this data with a former estimator, Tristan Wilson, who is the CEO of Edgevanta and a domain expert, I setteled on four primary variables that effect payitem unit prices (must balance context vs dimensionality in KNN Models): \n",
    "- __Location__: contracts in similar locations within a state tend to align/bid similarly\n",
    "- __Contract Outline__: types of payitems featured in the contract, the type of work to be completed, and the group of contractors that intend to/have bid on the job.\n",
    "- __Quantity__: the quantity associated with the payitem. Contractors tend to change their pricing structure based on the volume required for the contract\n",
    "- __Contract Size in Dollars__: really large contracts have different pricing structures than smaller contracts e.g., \\\\$50 million contract vs \\\\$5 million contract\n",
    "\n",
    "Like most ML problems, not only do the variables that you select matter, but also the means by which you decide to represent them. As a result, I used the following representation for the aforementioned variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90938de3-502a-49de-9622-b3c0752c56d2",
   "metadata": {},
   "source": [
    "- __Location as Distance__: each contract has an associated county such as Pitt in Fig. 4. Therefore, one can query Google Maps via an api to capture latitude and longitude information. From there, you can calculate the distance from one contract's location to any other contract's location, which benefits the latter KNN Regression model. The code below performs this task. Note that the returned coordinates can be confirmed by plotting them on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de395884-9cd3-498e-861e-37afb5f624cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "\n",
    "import folium\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from folium.plugins import MarkerCluster\n",
    "from tqdm import tqdm\n",
    "\n",
    "def combine_csv_files_to_single_df(dir: Path) -> pd.DataFrame:\n",
    "    files = dir.glob(\"*.csv\")\n",
    "    df = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def query_google_maps(county):\n",
    "    load_dotenv()\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    max_retries = 5\n",
    "    initial_delay, max_delay = 1, 60\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                base_url,\n",
    "                params={\n",
    "                    \"address\": f\"{county} County, North Carolina, USA\",\n",
    "                    \"key\": os.getenv(\"GOOGLE_MAPS_API_KEY\"),\n",
    "                },\n",
    "                timeout=5,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if data[\"status\"] == \"OK\":\n",
    "                location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "                return (location[\"lat\"], location[\"lng\"])\n",
    "            elif data[\"status\"] == \"ZERO_RESULTS\":\n",
    "                print(f\"No results found for {county} County\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"Error for {county} County: {data['status']}\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout error for {county} County\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error for {county} County: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for {county} County: {e}\")\n",
    "        # Exponential backoff with jitter\n",
    "        delay = min(\n",
    "            initial_delay * (2**attempt) + random.uniform(0, 1), max_delay\n",
    "        )\n",
    "        print(f\"Retrying {county} County in {delay:.2f} seconds...\")\n",
    "        time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_county_coordinates(counties: list):\n",
    "    county_coords = {}\n",
    "    for county in tqdm(counties, desc=\"Querying Google Maps\"):\n",
    "        coords = query_google_maps(county)\n",
    "        if coords:\n",
    "            county_coords[county] = coords\n",
    "        time.sleep(0.1)  # Delay between requests\n",
    "    return county_coords\n",
    "\n",
    "\n",
    "def create_map(county_coordinates: dict) -> None:\n",
    "    m = folium.Map(location=county_coordinates[\"All Counties\"], zoom_start=6)\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "    for county, coords in county_coordinates.items():\n",
    "        folium.Marker(\n",
    "            location=coords,\n",
    "            popup=f\"{county} County\",\n",
    "            tooltip=f\"{county} County\",\n",
    "        ).add_to(marker_cluster)\n",
    "    title_html = \"\"\"\n",
    "        <h3 align=\"center\" style=\"font-size:20px\"><b>North Carolina Counties</b></h3>\n",
    "        \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    m.save(\"county_coords/nc_counties_map.html\")\n",
    "    print(\n",
    "        \"Map created successfully and saved as county_coords/nc_counties_map.html\"\n",
    "    )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19eb817-fe74-40e8-8066-8f167daa2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "contracts_df = combine_csv_files_to_single_df(cwd / \"nc_csv\")\n",
    "\n",
    "# Process each county and get coordinates\n",
    "counties = contracts_df[\"County\"].unique().tolist()\n",
    "counties = [county.lower().title().split(\",\") for county in counties]\n",
    "counties = [county[0].strip() for county in counties]\n",
    "print(f\"Counties: {counties}\")\n",
    "\n",
    "# Load or create county coordinates\n",
    "if not Path(\"county_coords/nc_county_coords.csv\").exists():\n",
    "    county_coords = get_county_coordinates(counties)\n",
    "    county_coords[\"All Counties\"] = (\n",
    "        mean([coord[0] for coord in county_coords.values()]),\n",
    "        mean([coord[1] for coord in county_coords.values()]),\n",
    "    )\n",
    "else:\n",
    "    county_coords = {}\n",
    "    with open(\"county_coords/nc_county_coords.csv\", \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            county_coords[row[0]] = (float(row[1]), float(row[2]))\n",
    "\n",
    "# Save county coordinates\n",
    "Path(\"county_coords\").mkdir(exist_ok=True)\n",
    "with open(\"county_coords/nc_county_coords.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"County\", \"Latitude\", \"Longitude\"])\n",
    "    for county, coords in county_coords.items():\n",
    "        writer.writerow([county, *coords])\n",
    "\n",
    "csv_files = list(cwd.glob(\"nc_csv/*.csv\"))\n",
    "for csv_file in csv_files:\n",
    "    single_contract_df = pd.read_csv(csv_file)\n",
    "    # Initialize new columns\n",
    "    single_contract_df[\"Latitude\"] = None\n",
    "    single_contract_df[\"Longitude\"] = None\n",
    "    for idx, row in single_contract_df.iterrows():\n",
    "        county_list = row[\"County\"].lower().title().split(\",\")\n",
    "        county_list = [c.strip() for c in county_list]\n",
    "\n",
    "        if len(county_list) == 1:\n",
    "            # Single county\n",
    "            county = county_list[0]\n",
    "            if county in county_coords:\n",
    "                lat, lon = county_coords[county]\n",
    "                single_contract_df.at[idx, \"Latitude\"] = lat\n",
    "                single_contract_df.at[idx, \"Longitude\"] = lon\n",
    "        else:\n",
    "            # Multiple counties - take average\n",
    "            lats = []\n",
    "            lons = []\n",
    "            for county in county_list:\n",
    "                if county in county_coords:\n",
    "                    lat, lon = county_coords[county]\n",
    "                    lats.append(lat)\n",
    "                    lons.append(lon)\n",
    "            if (\n",
    "                lats and lons\n",
    "            ):  # If we found coordinates for at least one county\n",
    "                single_contract_df.at[idx, \"Latitude\"] = mean(lats)\n",
    "                single_contract_df.at[idx, \"Longitude\"] = mean(lons)\n",
    "    single_contract_df.to_csv(csv_file, index=False)\n",
    "\n",
    "create_map(county_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed901ea-3626-4bba-83f2-cb5a3d3e2dfa",
   "metadata": {},
   "source": [
    "__Figure 5: North Carolina County Locations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b068820b-5141-4fdc-82c3-d8ff24c5c0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"600\"\n",
       "            src=\"./county_coords/nc_counties_map.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x107876ea0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='./county_coords/nc_counties_map.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2593ab-6775-47da-9bbb-c8a0ca7f1301",
   "metadata": {},
   "source": [
    "- __Contract Outline__ - although a multi-hot encoded vector could represent the pay items that are present in a contract, the high dimensionality and sparsity resulting from thousands of payitems prove difficult for a KNN based algorithm. Futhermore, a multi-hot encoded vector does not account for the descriptions of the payitems, contract, or the remaining textual information present in the contract, which would provide further context for any algorithm. To tack this challenge, I relied a 1024-dimensional texual embedding to represent a contract and used cosine similarity to relate various contracts i.e., [0,1] scale, which is then a compatible form of 'distance' for a KNN model dimension. To create the embedding, I used an API call to [JINA](https://jina.ai/embeddings/), which is a state-of-the-art (SOTA) embedding model for textual and visual information. (You can read their paper [here](https://arxiv.org/abs/2409.10173)). The API call then returns the embedding vectors for each contract as a json file, which I can then parse and compare contracts via cosine similarity between two vectors. See the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118d1fa-4454-4a64-9a70-2280da76574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def generate_text_string(contract_df: pd.DataFrame) -> str:\n",
    "    text_columns = [\n",
    "        \"Item Description\",\n",
    "        \"Item Description Subset\",\n",
    "        \"Quantity\",\n",
    "        \"Unit\",\n",
    "    ]\n",
    "    proposal_type = contract_df.at[0, \"Proposal Type\"]\n",
    "    proposal_description = contract_df.at[0, \"Proposal Description\"]\n",
    "    item_types = contract_df[\"Item Type\"].unique().tolist()\n",
    "    item_types = \", \".join(item_types)\n",
    "    unique_item_info = contract_df[text_columns].astype(str).drop_duplicates()\n",
    "    unique_item_info.replace(\n",
    "        {\"Item Description Subset\": {\"nan\": \"\"}}, inplace=True\n",
    "    )\n",
    "    item_description_list = unique_item_info[\"Item Description\"].tolist()\n",
    "    item_description_subset_list = unique_item_info[\n",
    "        \"Item Description Subset\"\n",
    "    ].tolist()\n",
    "    quantity_list = unique_item_info[\"Quantity\"].tolist()\n",
    "    unit_list = unique_item_info[\"Unit\"].tolist()\n",
    "    vendor_name_list = contract_df[\"Vendor Name\"].unique().tolist()\n",
    "\n",
    "    total_string = \" \".join(\n",
    "        [\n",
    "            proposal_type,\n",
    "            proposal_description,\n",
    "            item_types,\n",
    "        ]\n",
    "    )\n",
    "    total_string += \"\\n\" + \", \".join(vendor_name_list) + \"\\n\"\n",
    "    for (\n",
    "        item_description,\n",
    "        item_description_subset,\n",
    "        quantity,\n",
    "        unit,\n",
    "    ) in zip(\n",
    "        item_description_list,\n",
    "        item_description_subset_list,\n",
    "        quantity_list,\n",
    "        unit_list,\n",
    "    ):\n",
    "        total_string += \" \".join(\n",
    "            [\n",
    "                item_description.strip(),\n",
    "                item_description_subset.strip(),\n",
    "                quantity.strip(),\n",
    "                unit.strip(),\n",
    "            ]\n",
    "        )\n",
    "        total_string += \"\\n\"\n",
    "    total_string += \", \".join(vendor_name_list) + \"\\n\"\n",
    "    total_string = total_string.replace(\"\\n\", \"\")\n",
    "    return total_string\n",
    "\n",
    "\n",
    "def rename_json_index(response: dict, csv_files: list):\n",
    "    for i, csv_embedding in enumerate(response[\"data\"]):\n",
    "        csv_embedding[\"index\"] = f\"{csv_files[i]}\"\n",
    "    return response\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_path = Path(\"nc_csv\")\n",
    "    texts = []\n",
    "    csv_files = [csv_file.stem for csv_file in sorted(data_path.glob(\"*.csv\"))]\n",
    "    tokens = 0\n",
    "\n",
    "    for csv_file in sorted(data_path.glob(\"*.csv\")):\n",
    "        contract_df = pd.read_csv(csv_file)\n",
    "        text = generate_text_string(contract_df)\n",
    "        texts.append(text)\n",
    "        tokens += len(text.split())\n",
    "\n",
    "    url = \"https://api.jina.ai/v1/embeddings\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"YOUR API KEY HERE\",\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"jina-embeddings-v3\",\n",
    "        \"task\": \"text-matching\",\n",
    "        \"late_chunking\": False,\n",
    "        \"dimensions\": 1024,\n",
    "        \"embedding_type\": \"float\",\n",
    "        \"input\": [{\"text\": text} for text in texts],\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "        response.raise_for_status()  # This will raise an HTTPError for 4xx/5xx status codes\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        # Get the actual response from the exception\n",
    "        response = e.response\n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        print(f\"Reason: {response.reason}\")\n",
    "        print(f\"Response Headers: {response.headers}\")\n",
    "        print(f\"Response Content: {response.text}\")\n",
    "    with open(\"original_jina_embedding_api_response.json\", \"w\") as f:\n",
    "        json.dump(response.json(), f, indent=2)\n",
    "    json_response = response.json()\n",
    "    modified_response = rename_json_index(json_response, csv_files)\n",
    "\n",
    "    with open(\"jina_embedding_proposalIndex_response.json\", \"w\") as f:\n",
    "        json.dump(modified_response, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69621f-e865-45e6-a9e8-e69dd33f0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080d4dd-2654-45af-988a-ea0fb8657076",
   "metadata": {},
   "source": [
    "__Figure 6: JSON File format containing the embeddings for the textual information in each contract's csv file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d840d89-b166-4477-97f1-ba65099a21b6",
   "metadata": {},
   "source": [
    "![JSON_Embedding_Response](./imgs/Embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7be465-75a3-48b7-a6b2-0a9a07ec4b77",
   "metadata": {},
   "source": [
    "- __Quantity__ - the quantity associated with each payitem for a given contract was already present in the csv files and could be taken as they were"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbad541-25be-4afd-acd7-3808bfdc63e1",
   "metadata": {},
   "source": [
    "- __Contract Size in Dollars__ - multiplied the payitem unit prices by their associated quantities and then summed to get total contract size. Each payitem within that contract is then associated with that contract size. This works great for past contracts that have already been bid on i.e., unit prices released after contract is rewarded. However, for an inference contract that is yet to be bid on, unit prices are not present and can't be used to determine contract size in dollars. However, total contract size can be estimated by taking a global average, across all contracts, of each payitem's unit price, weighted by quantity, and multiplying that by the respective quantity present in the inference contract. Although not perfect, it serves as a good esitimation for the expected contract size in dollars. `global_weighted_avg_C204988.csv` contains an example estimation of contract size in dollars for an upcoming contract C204988. The code below accomplishes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31f304-516e-4893-afff-dc30e7128e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def combine_csv_files_to_single_df(dir: Path) -> pd.DataFrame:\n",
    "    files = dir.glob(\"*.csv\")\n",
    "    df = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_extension_total_column(csv_path: Path) -> pd.DataFrame:\n",
    "    rankings_path = Path(\"contract_rankings\")\n",
    "    rankings_path.mkdir(exist_ok=True)\n",
    "    contract_df = pd.read_csv(csv_path)\n",
    "    contract_id = str(contract_df.at[0, \"Proposal\"]).strip()\n",
    "    contractor_groups = contract_df.groupby(\"Vendor Name\")\n",
    "    contract_rankings = dict()\n",
    "    for contractor_name, group in contractor_groups:\n",
    "        contract_df.loc[group.index, \"Total Contract Amount\"] = int(\n",
    "            group[\"Extension\"].sum()\n",
    "        )\n",
    "        contract_rankings[contractor_name] = int(group[\"Extension\"].sum())\n",
    "\n",
    "    contract_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Save dict to csv\n",
    "    rankings_df = pd.DataFrame(\n",
    "        contract_rankings.items(),\n",
    "        columns=[\"Vendor Name\", \"Total Contract Amount\"],\n",
    "    )\n",
    "    rankings_df = rankings_df.sort_values(\n",
    "        \"Total Contract Amount\", ascending=True\n",
    "    )\n",
    "    rankings_df.to_csv(\n",
    "        rankings_path / f\"{contract_id}_rankings.csv\", index=False\n",
    "    )\n",
    "\n",
    "    return contract_df, rankings_df\n",
    "\n",
    "\n",
    "def compute_payitem_weighted_averages(combined_df: pd.DataFrame) -> None:\n",
    "    weighted_path = Path(\"payitem_weightedaverages\")\n",
    "    weighted_path.mkdir(exist_ok=True)\n",
    "    combined_df[\"Item\"] = combined_df[\"Item\"].str.strip()\n",
    "    payitem_groups = combined_df.groupby(\"Item\")\n",
    "    weighted_averages = dict()\n",
    "    for payitem, group in payitem_groups:\n",
    "        if len(group.index) == 1:\n",
    "            weighted_averages[payitem] = group[\"Unit Price\"].values[0]\n",
    "        else:\n",
    "            weighted_averages[payitem] = np.round(\n",
    "                np.average(group[\"Unit Price\"], weights=group[\"Quantity\"]), 2\n",
    "            )\n",
    "    weighted_averages_df = pd.DataFrame(\n",
    "        weighted_averages.items(),\n",
    "        columns=[\"Item\", \"Unit Price Weighted Average\"],\n",
    "    )\n",
    "    weighted_averages_df = weighted_averages_df.sort_values(\n",
    "        \"Item\", ascending=True\n",
    "    )\n",
    "    weighted_averages_df.to_csv(\n",
    "        weighted_path / \"weighted_averages.csv\", index=False\n",
    "    )\n",
    "\n",
    "\n",
    "def estimate_contract_extension(inference_df: pd.DataFrame) -> int:\n",
    "    weighted_averages = pd.read_csv(\n",
    "        \"payitem_weightedaverages/weighted_averages.csv\"\n",
    "    )\n",
    "    estimated_extension = 0\n",
    "    for item, quantity in zip(\n",
    "        inference_df[\"Item\"].tolist(), inference_df[\"Quantity\"].tolist()\n",
    "    ):\n",
    "        if item not in weighted_averages[\"Item\"].values or pd.isna(\n",
    "            weighted_averages[weighted_averages[\"Item\"] == item][\n",
    "                \"Unit Price Weighted Average\"\n",
    "            ].values[0]\n",
    "        ):\n",
    "            print(f\"Item {item} not found in weighted averages\")\n",
    "            continue\n",
    "        weighted_average = weighted_averages[weighted_averages[\"Item\"] == item][\n",
    "            \"Unit Price Weighted Average\"\n",
    "        ].values[0]\n",
    "        estimated_extension += float(weighted_average) * float(quantity)\n",
    "    return round(estimated_extension, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c01a8-e452-4d9b-a81b-846ec547d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Path(\"nc_csv\")\n",
    "shutil.rmtree(\"contract_rankings\", ignore_errors=True)\n",
    "for file in dir.glob(\"*.csv\"):\n",
    "    contract_df, rankings_df = add_extension_total_column(file)\n",
    "combined_df = combine_csv_files_to_single_df(dir)\n",
    "compute_payitem_weighted_averages(combined_df)\n",
    "inference_df = pd.read_csv(\n",
    "    \"~/Downloads/Proposal_NCDOT_L241217_C204988_003.csv\"\n",
    ")\n",
    "estimated_extension = estimate_contract_extension(inference_df)\n",
    "print(f\"Estimated extension: {estimated_extension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d652bbb-d0ed-401e-b44e-d33a03e39cb3",
   "metadata": {},
   "source": [
    "With the four variables/dimensions transformed into their respective representations, a KNN Regression model could be built to predict a contract's unit prices for payitems based on the contract's values for the four aforementioned input variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d40d41-de02-44c2-ac14-183432b794df",
   "metadata": {},
   "source": [
    "## Unit Price Prediction - KNN Regression with Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecaa5da-ac1c-4319-a63f-4da59575fa54",
   "metadata": {},
   "source": [
    "Helper functions for the main model loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cb615-cca2-4d15-a5b2-569a81433494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from functools import cache, lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic as GD\n",
    "\n",
    "def estimate_inference_contract_size(\n",
    "    total_df: pd.DataFrame,\n",
    "    inference_contract_data: Tuple[List[str], List[float]],\n",
    "    contract: str,\n",
    ") -> float:\n",
    "    total_size = 0\n",
    "    payitems, quantities = inference_contract_data\n",
    "    with open(f\"global_weighted_avg_{contract}.csv\", \"w\") as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(\n",
    "            [\n",
    "                \"Payitem\",\n",
    "                \"Global Weighted Avg\",\n",
    "                \"Proposal Quantity\",\n",
    "                \"Global Unit Price Prediction\",\n",
    "            ]\n",
    "        )\n",
    "        for payitem, quantity in zip(payitems, quantities):\n",
    "            payitem_df = total_df[total_df[\"Item\"] == payitem]\n",
    "            quantities = np.array(payitem_df[\"Quantity\"])\n",
    "            unit_prices = np.array(payitem_df[\"Unit Price\"])\n",
    "            if len(quantities) == 0 or np.all(quantities == 0):\n",
    "                continue\n",
    "            payitem_global_weighted_avg = np.round(\n",
    "                np.average(unit_prices, weights=quantities), 2\n",
    "            )\n",
    "\n",
    "            \"\"\"\n",
    "            print(\n",
    "                f\"Payitem: {payitem} - Weighted Avg: {payitem_global_weighted_avg}\"\n",
    "            )\n",
    "            \"\"\"\n",
    "\n",
    "            total_size += payitem_global_weighted_avg * quantity\n",
    "            global_unit_price_prediction = round(\n",
    "                payitem_global_weighted_avg * quantity, 2\n",
    "            )\n",
    "            csv_writer.writerow(\n",
    "                [\n",
    "                    payitem,\n",
    "                    payitem_global_weighted_avg,\n",
    "                    quantity,\n",
    "                    global_unit_price_prediction,\n",
    "                ]\n",
    "            )\n",
    "        csv_writer.writerow([\"\", \"\", \"Total\", round(total_size, 0)])\n",
    "    return total_size\n",
    "\n",
    "@cache\n",
    "def create_embedding_dict():\n",
    "    embedding_dict = {}\n",
    "    embedding_path = (\n",
    "        Path().cwd()\n",
    "        / \"embeddings\"\n",
    "        / \"jina_embedding_proposalIndex_response.json\"\n",
    "    )\n",
    "    with open(embedding_path, \"r\") as f:\n",
    "        embeddings = json.load(f)\n",
    "    data: List = embeddings[\"data\"]\n",
    "    for info_dict in data:\n",
    "        embedding_dict[info_dict[\"index\"]] = np.array(\n",
    "            info_dict[\"embedding\"]\n",
    "        ).tolist()\n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "@cache\n",
    "def load_embedding(contract_id: str):\n",
    "    embeddings = create_embedding_dict()\n",
    "    return embeddings.get(contract_id, None)\n",
    "\n",
    "\n",
    "contract_locations: Dict[str, Tuple[float, float]] = {}\n",
    "\n",
    "\n",
    "def get_contract_location(contract: str) -> Tuple[float, float]:\n",
    "    \"\"\"Cache and return contract locations\"\"\"\n",
    "    if contract not in contract_locations:\n",
    "        df = pd.read_csv(f\"nc_csv/{contract}.csv\")\n",
    "        contract_locations[contract] = (\n",
    "            df.at[0, \"Latitude\"],\n",
    "            df.at[0, \"Longitude\"],\n",
    "        )\n",
    "    return contract_locations[contract]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def load_distance(contract_one: str, contract_two: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute distance between contracts with multiple levels of caching:\n",
    "    1. Contract locations are cached to avoid repeated CSV reads\n",
    "    2. Distance computations are cached with lru_cache\n",
    "    3. Symmetric computation (A->B same as B->A)\n",
    "    \"\"\"\n",
    "    # Sort contracts to ensure symmetric caching (A->B same as B->A)\n",
    "    if contract_one > contract_two:\n",
    "        contract_one, contract_two = contract_two, contract_one\n",
    "\n",
    "    location_one = get_contract_location(contract_one)\n",
    "    location_two = get_contract_location(contract_two)\n",
    "\n",
    "    return float(GD(location_one, location_two).miles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e7c91-8df5-4761-8b99-b0826dba877d",
   "metadata": {},
   "source": [
    "__KNN Regression Notes__\n",
    "-  Each of the four variables are scaled to improve KNN Regression performance (degrades with unequal dimension scales)\n",
    "-  Kernel PCA, with a Radial Basis Function, was used to transform the data into a new feature space that better captures nonlinear relationships between variables, before using these transformed features for prediction. The dimensionality of this newer space may be greater than the original 4 dimensions considered but will more accurately capture the non-linear relationships of the data and distances between points.\n",
    "-  These predictions are made for each contractor's payitems in the contract, placed in a python dictionary, and then saved to an excel sheet within the `InferenceResults` directory\n",
    "-  The dataset size for a contractor's particular payitem over the 2.5yr window considered ranges from ~1-250 examples. Such small dataset sizes are difficult to train DL algorithms. Consequently, a transductive model approach is more adequate because it makes its predictions directly from the training data without trying to learn a general function that maps inputs to outputs such as Neural Networks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2521418-7e6f-42c6-bd10-e0c09e7bb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from geopy.distance import geodesic as GD\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "from calcs import predict_with_confidence\n",
    "from contract_locations import query_google_maps\n",
    "from inferenceExcel import excel_predictions\n",
    "from model_helpers import (\n",
    "    combine_csv_files,\n",
    "    create_embedding_dict,\n",
    "    estimate_inference_contract_size,\n",
    "    load_distance,\n",
    ")\n",
    "from models import ImprovedKNNRegression\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceProposal:\n",
    "    proposal: str\n",
    "    contractors: list[str]\n",
    "    embedding: np.ndarray\n",
    "    location: tuple\n",
    "    estimated_contract_size: float = 0.0\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dict: dict,\n",
    "        combined_df: pd.DataFrame,\n",
    "        inference_proposal: InferenceProposal,\n",
    "    ):\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.combined_df = combined_df\n",
    "        self.inference_proposal = inference_proposal\n",
    "        self.inference()\n",
    "\n",
    "    def inference(self):\n",
    "        os.makedirs(\"InferenceResults\", exist_ok=True)\n",
    "        output: dict = {}\n",
    "        output.setdefault(self.inference_proposal.proposal, {})\n",
    "        contractor_list = self.inference_proposal.contractors\n",
    "        payitems_df = pd.read_csv(\n",
    "            f\"InferenceProposals/{self.inference_proposal.proposal}.csv\"\n",
    "        )\n",
    "        payitems_df[\"Item\"] = payitems_df[\"Item\"].str.strip()\n",
    "        print(payitems_df)\n",
    "        est_inference_contract_size = estimate_inference_contract_size(\n",
    "            self.combined_df,\n",
    "            (\n",
    "                payitems_df[\"Item\"].tolist(),\n",
    "                payitems_df[\"Quantity\"].astype(float).tolist(),\n",
    "            ),\n",
    "            self.inference_proposal.proposal,\n",
    "        )\n",
    "        self.inference_proposal.estimated_contract_size = (\n",
    "            est_inference_contract_size\n",
    "        )\n",
    "        print(contractor_list)\n",
    "        for contractor in contractor_list:\n",
    "            output[self.inference_proposal.proposal].setdefault(contractor, {})\n",
    "            for payitem in payitems_df[\"Item\"]:\n",
    "                print(f\"Starting inference for {payitem} in {contractor}\")\n",
    "                inference_payitem_data = payitems_df[\n",
    "                    payitems_df[\"Item\"] == payitem\n",
    "                ]\n",
    "                line_item = inference_payitem_data[\"Line\"]\n",
    "                payitem_desc = inference_payitem_data[\"Description\"]\n",
    "                inference_quantity = inference_payitem_data[\"Quantity\"]\n",
    "                inference_unit = inference_payitem_data[\"Unit\"].iloc[0]\n",
    "                historic_data = self.combined_df[\n",
    "                    (\n",
    "                        self.combined_df[\"Proposal\"].astype(str)\n",
    "                        != self.inference_proposal.proposal\n",
    "                    )\n",
    "                    & (self.combined_df[\"Vendor Name\"] == contractor)\n",
    "                    & (self.combined_df[\"Item\"] == payitem)\n",
    "                ]\n",
    "                if historic_data.empty:\n",
    "                    print(f\"No data found for {payitem} in {contractor}\")\n",
    "                    for line, quantity, desc in zip(\n",
    "                        line_item, inference_quantity, payitem_desc\n",
    "                    ):\n",
    "                        output[self.inference_proposal.proposal][contractor][\n",
    "                            f\"{payitem}:{line}:{desc}\"\n",
    "                        ] = [\n",
    "                            \"N/A\",\n",
    "                            quantity,\n",
    "                            \"N/A\",\n",
    "                            0,\n",
    "                            \"N/A\",\n",
    "                            [],\n",
    "                            inference_unit,\n",
    "                        ]\n",
    "                    continue\n",
    "\n",
    "                historic_data = historic_data.copy()\n",
    "\n",
    "                historic_data[\"Similarity\"] = historic_data[\"Proposal\"].apply(\n",
    "                    lambda x: np.dot(\n",
    "                        self.inference_proposal.embedding,\n",
    "                        self.embedding_dict[x],\n",
    "                    )\n",
    "                    / (\n",
    "                        np.linalg.norm(self.inference_proposal.embedding)\n",
    "                        * np.linalg.norm(self.embedding_dict[x])\n",
    "                    )\n",
    "                )\n",
    "                # compute distances\n",
    "                historic_data[\"Distance\"] = historic_data.apply(\n",
    "                    lambda row: float(\n",
    "                        GD(\n",
    "                            self.inference_proposal.location,\n",
    "                            (row[\"Latitude\"], row[\"Longitude\"]),\n",
    "                        ).miles\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "                number_payitem_occurances = len(historic_data)\n",
    "                # Scaling\n",
    "                quantity_scaler = Pipeline(\n",
    "                    [\n",
    "                        (\"robust\", RobustScaler()),\n",
    "                        (\"minmax\", MinMaxScaler()),\n",
    "                    ]\n",
    "                )\n",
    "                distance_scaler = Pipeline(\n",
    "                    [\n",
    "                        (\"robust\", RobustScaler()),\n",
    "                        (\"minmax\", MinMaxScaler()),\n",
    "                    ]\n",
    "                )\n",
    "                contract_dollar_size_scaler = Pipeline(\n",
    "                    [\n",
    "                        (\"robust\", RobustScaler()),\n",
    "                        (\"minmax\", MinMaxScaler()),\n",
    "                    ]\n",
    "                )\n",
    "                features = [\n",
    "                    \"Quantity Scaled\",\n",
    "                    \"Distance Scaled\",\n",
    "                    \"Similarity\",\n",
    "                    \"Total Contract Amount Scaled\",\n",
    "                ]\n",
    "\n",
    "                historic_data[\"Quantity Scaled\"] = (\n",
    "                    quantity_scaler.fit_transform(historic_data[[\"Quantity\"]])\n",
    "                )\n",
    "                historic_data[\"Distance Scaled\"] = (\n",
    "                    distance_scaler.fit_transform(historic_data[[\"Distance\"]])\n",
    "                )\n",
    "                historic_data[\"Total Contract Amount Scaled\"] = (\n",
    "                    contract_dollar_size_scaler.fit_transform(\n",
    "                        historic_data[[\"Total Contract Amount\"]]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                inference_quantity = pd.DataFrame(\n",
    "                    inference_payitem_data[\"Quantity\"], columns=[\"Quantity\"]\n",
    "                )\n",
    "\n",
    "                contract_amount = pd.DataFrame(\n",
    "                    [self.inference_proposal.estimated_contract_size],\n",
    "                    columns=[\"Total Contract Amount\"],\n",
    "                )\n",
    "\n",
    "                quantity_scaled = quantity_scaler.transform(inference_quantity)\n",
    "                contract_amount_scaled = contract_dollar_size_scaler.transform(\n",
    "                    contract_amount\n",
    "                )\n",
    "\n",
    "                inference_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"Quantity Scaled\": quantity_scaled.ravel(),\n",
    "                    }\n",
    "                )\n",
    "                inference_df[\"Distance Scaled\"] = 0\n",
    "                inference_df[\"Similarity\"] = 1\n",
    "                inference_df[\"Total Contract Amount Scaled\"] = (\n",
    "                    contract_amount_scaled.item()\n",
    "                )\n",
    "                print(f\"{'Inference data':-^50}\")\n",
    "                print(inference_df)\n",
    "                print(\"\\n\")\n",
    "\n",
    "                inference_df = inference_df[features]\n",
    "\n",
    "                kpca = KernelPCA(\n",
    "                    n_components=None, kernel=\"rbf\", random_state=42\n",
    "                )\n",
    "                historic_kpca = kpca.fit_transform(historic_data[features])\n",
    "                inference_kpca = kpca.transform(inference_df[features])\n",
    "\n",
    "                eigenvalues = kpca.eigenvalues_\n",
    "                total_variance = np.sum(eigenvalues)\n",
    "\n",
    "                if total_variance > 0:\n",
    "                    variance_ratio = eigenvalues / total_variance\n",
    "                    cumulative_variance_ratio = np.cumsum(variance_ratio)\n",
    "                    if np.any(cumulative_variance_ratio > 0.95):\n",
    "                        n_components = (\n",
    "                            np.argmax(cumulative_variance_ratio > 0.95) + 1\n",
    "                        )\n",
    "                    else:\n",
    "                        n_components = len(cumulative_variance_ratio)\n",
    "                    historic_kpca = historic_kpca[:, :n_components]\n",
    "                    inference_kpca = inference_kpca[:, :n_components]\n",
    "                    historic_kpca_df = pd.DataFrame(\n",
    "                        historic_kpca,\n",
    "                        columns=[f\"PC{i+1}\" for i in range(n_components)],\n",
    "                    )\n",
    "                    inference_kpca_df = pd.DataFrame(\n",
    "                        inference_kpca,\n",
    "                        columns=[f\"PC{i+1}\" for i in range(n_components)],\n",
    "                    )\n",
    "\n",
    "                    model_used = \"KNN with Kernel PCA\"\n",
    "                    knn = ImprovedKNNRegression(\n",
    "                        historic_kpca_df, historic_data[\"Unit Price\"]\n",
    "                    )\n",
    "                    predictions, confidences, details = predict_with_confidence(\n",
    "                        knn,\n",
    "                        inference_kpca_df,\n",
    "                        historic_kpca_df,\n",
    "                        k=knn.best_k,\n",
    "                    )\n",
    "                    distances, indices = knn.model.kneighbors(\n",
    "                        inference_kpca_df,\n",
    "                        n_neighbors=knn.best_k,\n",
    "                        return_distance=True,\n",
    "                    )\n",
    "\n",
    "                    for prediction, confidence in zip(predictions, confidences):\n",
    "                        print(f\"{model_used:-^50}\")\n",
    "                        print(f\"Pay item: {payitem:10s}\")\n",
    "                        print(f\"Description: {payitem_desc}\")\n",
    "                        print(f\"Prediction: {prediction:10.2f}\")\n",
    "                        print(f\"Confidence: {confidence:10.2f}\")\n",
    "                        print(f\"Shape of input data: {historic_kpca.shape}\")\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Warning: All eigenvalues are zero or very close to zero. Using original features.\"\n",
    "                    )\n",
    "                    model_used = \"KNN with original features\"\n",
    "                    knn = ImprovedKNNRegression(\n",
    "                        historic_data[features],\n",
    "                        historic_data[\"Unit Price\"],\n",
    "                    )\n",
    "                    predictions, confidences, details = predict_with_confidence(\n",
    "                        knn,\n",
    "                        inference_df,\n",
    "                        historic_data[features],\n",
    "                        k=knn.best_k,\n",
    "                    )\n",
    "                    distances, indices = knn.model.kneighbors(\n",
    "                        inference_df,\n",
    "                        n_neighbors=knn.best_k,\n",
    "                        return_distance=True,\n",
    "                    )\n",
    "                    for prediction, confidence in zip(predictions, confidences):\n",
    "                        print(f\"{model_used:-^50}\")\n",
    "                        print(f\"Pay item: {payitem:10s}\")\n",
    "                        print(f\"Description: {payitem_desc}\")\n",
    "                        print(f\"Prediction: {prediction:10.2f}\")\n",
    "                        print(f\"Confidence: {confidence:10.2f}\")\n",
    "                        print(\n",
    "                            f\"Shape of input data: {historic_data[features].shape}\"\n",
    "                        )\n",
    "                for (\n",
    "                    prediction,\n",
    "                    quantity,\n",
    "                    confidence,\n",
    "                    distance,\n",
    "                    index,\n",
    "                    line,\n",
    "                    description,\n",
    "                ) in zip(\n",
    "                    predictions,\n",
    "                    inference_quantity[\"Quantity\"].tolist(),\n",
    "                    confidences,\n",
    "                    distances,\n",
    "                    indices,\n",
    "                    line_item,\n",
    "                    payitem_desc,\n",
    "                ):\n",
    "                    print(f\"{'Historic data':-^50}\")\n",
    "                    print(historic_data)\n",
    "                    print(f\"{'Index':-^50}\")\n",
    "                    print(index)\n",
    "\n",
    "                    nearest_neighbors = (\n",
    "                        historic_data.iloc[index]\n",
    "                        .sort_values(\"Distance\")[\"Proposal\"]\n",
    "                        .tolist()\n",
    "                    )\n",
    "                    extension = round(quantity * prediction, 0)\n",
    "                    output[self.inference_proposal.proposal][contractor][\n",
    "                        f\"{payitem}:{line}:{description}\"\n",
    "                    ] = [\n",
    "                        round(prediction, 2),\n",
    "                        quantity,\n",
    "                        extension,\n",
    "                        number_payitem_occurances,\n",
    "                        round(confidence, 1),\n",
    "                        list(zip(distance, nearest_neighbors)),\n",
    "                        historic_data[\"Unit\"].iloc[0],\n",
    "                    ]\n",
    "                    nearest_neighbors = None\n",
    "            print(f\"Finished inference for {contractor}\")\n",
    "        print(f\"Finished inference for {self.inference_proposal.proposal}\")\n",
    "        excel_predictions(\n",
    "            output, f\"InferenceResults/{self.inference_proposal.proposal}.xlsx\"\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_text_string(\n",
    "    contract_df: pd.DataFrame, contractors: List[str], proposal_description: str\n",
    ") -> str:\n",
    "    text_columns = [\n",
    "        \"Description\",\n",
    "        \"Quantity\",\n",
    "        \"Unit\",\n",
    "    ]\n",
    "    unique_item_info = contract_df[text_columns].astype(str).drop_duplicates()\n",
    "    item_description_list = unique_item_info[\"Description\"].tolist()\n",
    "    quantity_list = unique_item_info[\"Quantity\"].tolist()\n",
    "    unit_list = unique_item_info[\"Unit\"].tolist()\n",
    "\n",
    "    total_string = proposal_description + \" \"\n",
    "    total_string += \", \".join(contractors)\n",
    "    for (\n",
    "        item_description,\n",
    "        quantity,\n",
    "        unit,\n",
    "    ) in zip(\n",
    "        item_description_list,\n",
    "        quantity_list,\n",
    "        unit_list,\n",
    "    ):\n",
    "        total_string += \" \".join(\n",
    "            [\n",
    "                item_description.strip(),\n",
    "                quantity.strip(),\n",
    "                unit.strip(),\n",
    "            ]\n",
    "        )\n",
    "        total_string += \", \"\n",
    "    total_string += \", \".join(contractors)\n",
    "    total_string = total_string.replace(\"\\n\", \"\")\n",
    "    return total_string\n",
    "\n",
    "\n",
    "def generate_inference_proposal_embedding(\n",
    "    contract: str, vendors: List[str], proposal_description: str\n",
    ") -> np.ndarray:\n",
    "    inference_csv = Path(\"InferenceProposals\") / f\"{contract}.csv\"\n",
    "    proposal_df = pd.read_csv(inference_csv)\n",
    "    text = generate_text_string(proposal_df, vendors, proposal_description)\n",
    "    url = \"https://api.jina.ai/v1/embeddings\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"YOUR API KEY HERE\",\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"jina-embeddings-v3\",\n",
    "        \"task\": \"text-matching\",\n",
    "        \"late_chunking\": False,\n",
    "        \"dimensions\": 1024,\n",
    "        \"embedding_type\": \"float\",\n",
    "        \"input\": [{\"text\": text}],\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "        response.raise_for_status()  # This will raise an HTTPError for 4xx/5xx status codes\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        # Get the actual response from the exception\n",
    "        response = e.response\n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        print(f\"Reason: {response.reason}\")\n",
    "        print(f\"Response Headers: {response.headers}\")\n",
    "        print(f\"Response Content: {response.text}\")\n",
    "    response = response.json()\n",
    "    return response[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a04da-c70f-4453-b002-8530eb724875",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = create_embedding_dict()\n",
    "combined_df, _ = combine_csv_files(training=False)\n",
    "combined_df[\"Item\"] = combined_df[\"Item\"].str.strip()\n",
    "combined_df[\"Vendor Name\"] = combined_df[\"Vendor Name\"].str.strip()\n",
    "combined_df = combined_df.dropna(subset=[\"Unit Price\"])\n",
    "contractors_list = [\n",
    "    \"BARNHILL CONTRACTING CO\",\n",
    "    \"FSC II LLC DBA FRED SMITH COMPANY\",\n",
    "    \"HIGHLAND PAVING CO LLC\",\n",
    "]\n",
    "if not os.path.exists(\"pickles/C204988_inference_proposal.pkl\"):\n",
    "    print(\"Generating inference proposal\")\n",
    "    inference_proposal = InferenceProposal(\n",
    "        proposal=\"C204988\",\n",
    "        contractors=contractors_list,\n",
    "        embedding=generate_inference_proposal_embedding(\n",
    "            \"C204988\",\n",
    "            contractors_list,\n",
    "            \"GRADING, DRAINAGE, PAVING,SIGNALS, AND WALLS.\",\n",
    "        ),\n",
    "        location=query_google_maps(\"Cumberland\"),\n",
    "    )\n",
    "    with open(\"pickles/C204988_inference_proposal.pkl\", \"wb\") as f:\n",
    "        pickle.dump(inference_proposal, f)\n",
    "else:\n",
    "    print(\"Loading inference proposal from pickle\")\n",
    "    with open(\"pickles/C204988_inference_proposal.pkl\", \"rb\") as f:\n",
    "        inference_proposal = pickle.load(f)\n",
    "inference = Inference(embedding_dict, combined_df, inference_proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d9509-08fe-456e-b8d7-9a7e00b91f3a",
   "metadata": {},
   "source": [
    "__Code to produce Excel sheets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6e514-d189-4c10-a769-067e3eeaacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def excel_predictions(data, excel_file_path):\n",
    "    os.makedirs(os.path.dirname(excel_file_path), exist_ok=True)\n",
    "    rows = []\n",
    "    \"\"\"output[proposal][contractor][payitem_number][\n",
    "                                f\"{payitem}:{line_item}:{payitem_desc}\"\n",
    "                            ] = [\n",
    "                                round(prediction, 2),\n",
    "                                inference_quantity,\n",
    "                                extension,\n",
    "                                number_payitem_occurances,\n",
    "                                round(confidence, 1),\n",
    "                                list(zip(distance, nearest_neighbors)),\n",
    "                            ]\n",
    "    \"\"\"\n",
    "    for proposal, contractors in data.items():\n",
    "        for contractor, payitems in contractors.items():\n",
    "            for payitem_full, details in payitems.items():\n",
    "\n",
    "                payitem, line_item, payitem_desc = payitem_full.split(\":\", 2)\n",
    "                print(f\"Payitem: {payitem}\")\n",
    "                print(f\"Line Item: {line_item}\")\n",
    "                print(f\"Description: {payitem_desc}\")\n",
    "                (\n",
    "                    prediction,\n",
    "                    excel_quantity,\n",
    "                    extension,\n",
    "                    occurences,\n",
    "                    confidence,\n",
    "                    neighbors,\n",
    "                    unit,\n",
    "                ) = details\n",
    "                nearest_proposals = \", \".join(n[1] for n in neighbors)\n",
    "                print(\"Prediction: \", prediction)\n",
    "                print(\"Excel Quantity: \", excel_quantity)\n",
    "                print(\"Extension: \", extension)\n",
    "                print(\"Occurences: \", occurences)\n",
    "                print(\"Confidence: \", confidence)\n",
    "                print(\"Nearest Proposals: \", nearest_proposals)\n",
    "                print(\"Unit: \", unit)\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"Proposal\": proposal,\n",
    "                        \"Line\": line_item,\n",
    "                        \"Item\": payitem,\n",
    "                        \"Description\": payitem_desc,\n",
    "                        \"Contractor\": contractor,\n",
    "                        \"Unit Price\": prediction,\n",
    "                        \"Quantity\": excel_quantity,\n",
    "                        \"Extension\": extension,\n",
    "                        \"Occurences\": occurences,\n",
    "                        \"Nearest Proposals\": nearest_proposals,\n",
    "                        \"Unit\": unit,\n",
    "                    }\n",
    "                )\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(df.head())\n",
    "    pivot = pd.pivot_table(\n",
    "        df,\n",
    "        values=[\n",
    "            \"Unit Price\",\n",
    "            \"Extension\",\n",
    "            \"Occurences\",\n",
    "            \"Nearest Proposals\",\n",
    "        ],\n",
    "        index=[\"Proposal\", \"Line\", \"Item\", \"Quantity\", \"Unit\", \"Description\"],\n",
    "        columns=[\"Contractor\"],\n",
    "        aggfunc=\"first\",\n",
    "        sort=False,\n",
    "    )\n",
    "\n",
    "    # Sort by Line number\n",
    "    pivot = pivot.reset_index()  # Make the index into columns\n",
    "    pivot[\"Line\"] = pd.to_numeric(pivot[\"Line\"])  # Convert Line to numeric\n",
    "    pivot = pivot.sort_values(\"Line\")  # Sort by Line\n",
    "    pivot = pivot.set_index(\n",
    "        [\"Proposal\", \"Line\", \"Item\", \"Quantity\", \"Unit\", \"Description\"]\n",
    "    )  # Reset the index\n",
    "\n",
    "    # Reorder and sort columns\n",
    "    pivot = pivot.reorder_levels([1, 0], axis=1)\n",
    "    pivot = pivot.sort_index(axis=1, level=0)\n",
    "    metric_order = [\n",
    "        \"Unit Price\",\n",
    "        \"Extension\",\n",
    "        \"Occurences\",\n",
    "        \"Nearest Proposals\",\n",
    "    ]\n",
    "    pivot = pivot.reindex(metric_order, axis=1, level=1)\n",
    "\n",
    "    # Create empty total row with the same structure as pivot\n",
    "    total_row = pd.DataFrame(\n",
    "        index=pd.MultiIndex.from_tuples(\n",
    "            [(\"Total\", \"\", \"\", \"\", \"\", \"\")], names=pivot.index.names\n",
    "        ),\n",
    "        columns=pivot.columns,\n",
    "        data=[[\"\" for _ in pivot.columns]],  # Initialize all cells as empty\n",
    "    )\n",
    "\n",
    "    # Calculate and set totals for each contractor's Extension column\n",
    "    for contractor in df[\"Contractor\"].unique():\n",
    "        print(f\"\\nCalculating total for {contractor}\")\n",
    "\n",
    "        # Get all extensions for this contractor\n",
    "        contractor_data = df[df[\"Contractor\"] == contractor]\n",
    "        contractor_extensions = contractor_data[\"Extension\"]\n",
    "\n",
    "        print(\"All extensions:\", contractor_extensions.tolist())\n",
    "\n",
    "        # Sum only numeric values\n",
    "        total = 0\n",
    "        for ext in contractor_extensions:\n",
    "            if ext != \"N/A\" and isinstance(ext, (int, float)):\n",
    "                total += ext\n",
    "\n",
    "        # Set the total in the correct column\n",
    "        col = (contractor, \"Extension\")\n",
    "        if col in total_row.columns:\n",
    "            total_row.loc[(\"Total\", \"\", \"\", \"\", \"\", \"\"), col] = total\n",
    "\n",
    "    # Combine pivot table with total row\n",
    "    result = pd.concat([pivot, total_row])\n",
    "\n",
    "    with pd.ExcelWriter(excel_file_path, engine=\"openpyxl\") as writer:\n",
    "        result.to_excel(writer, sheet_name=\"Organized Data\")\n",
    "        df.to_excel(writer, sheet_name=\"Raw Data\", index=False)\n",
    "    print(f\"Excel file saved to {excel_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370b920-fd07-4ce2-b6f9-a523ef41554e",
   "metadata": {},
   "source": [
    "The excel sheet depicted in Figure 7 illustrates the prediction result for the payitems in a given contract. Using this information, contractors can gain insight into how their competitors on a project will likely bid. Additionally, the interpretibility of KNN models enables one to discover the nearest neighbors that contributed to a prediction. Therefore, a contractor can look at the most similar proposals that will likely influence their competitors' bidding decisions. This enables a more interactive and explainable approach with the model's predictions instead of relying on a black box model frequently found in Deep Learning. See `InferenceResults/C204988.xlsx` for the full results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201716b-9acf-430a-8dcb-972e3066aa81",
   "metadata": {},
   "source": [
    "__Figure 7: Example Contract Excel Sheet Ouput__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28e911-3493-479f-90bf-9a7843e7b7e6",
   "metadata": {},
   "source": [
    "![ExcelOutput](./imgs/ExcelOutput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0594cd-a4d4-4d0b-961c-2b694832f139",
   "metadata": {},
   "source": [
    "## Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce03ab8-dd1b-479b-a787-6c6a6d8303a8",
   "metadata": {},
   "source": [
    "Given the small dataset sizes for each contractor's bidding history on a particular pay item, training a deep learning network is impractical. Therefore, using a transductive model such as KNN regression, which makes its predictions based on the training data, produces more realiable and interpretable results. The output of the model along with the most similar data points that influence the prediction enables estimators to explore the model's decision making and make corrections. Consequently, this tool can be viewed as not a replacement for estimators, but as an enhancement to their work flow (most project estimates take several days to research and prepare; this takes less than a minute!). \n",
    "\n",
    "For future work, an objective value/evalutaion metric should be developed to track and judge the model's performance over time e.g., average payitem prediction error, total contract prediction error, etc. Futhermore, the parameter space of variables that effect unit prices should be expanded and explored outside of the 4 variables that were considered thus far: Distance, Quantity, Contract Textual Similarity, and Contract Size in Dollars. Accounting for additional parameters will likely increase context and improve performance if it can be balanced through dimensionality reduction (PCA) and avoiding the curse of dimensionality that plagues KNN style models. \n",
    "\n",
    "Overall, this machine learning workflow captures, preprocesses, and analyzes the data necessary for contractors to win more work at the right price and reduce money left on the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c2da7-e7e8-448f-b74d-cea5caa69a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
